{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ae2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# Analysis script that uses visual source only for issue report classification         #\n",
    "# Section 5.3 Single-Source Approaches using Attachments Only                          #\n",
    "########################################################################################\n",
    "import re\n",
    "import pandas\n",
    "import numpy as np\n",
    "from time import gmtime, strftime, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ef8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The issues have been downloaded and saved as a csv file. \n",
    "# The following are the column names in the input data file, change them according to your file specifications \n",
    "CNAME_TEAMCODE = \"TEAMCODE\"\n",
    "CNAME_ATTACHMENT_TEXT = \"OCR_DATA\" # The images have been processed previously and results are stored in this column.\n",
    "CNAME_RECORD_TYPE = \"RECORDTYPE\"\n",
    "CNAME_STATUS = \"STATUS\"\n",
    "CNAME_YEAR_OPENED = \"CREATIONYEAR\" # year of creation of issue reports\n",
    "CNAME_MONTH_OPENED = \"CREATIONMONTH\" # month of creation of issue reports\n",
    "# filtering specifications \n",
    "FILTER_ISSUE_TYPE = 'ISSUE'\n",
    "FILTER_ISSUE_STATUS = 'CLOSED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38723951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectRecordsOpenedAtYearMonth(dataset, year, month):\n",
    "    \"\"\"\n",
    "    returns the issue records belonging to the input year and month\n",
    "    \"\"\"\n",
    "    d1 = dataset.groupby(CNAME_YEAR_OPENED).filter(lambda x: x.name == year)\n",
    "\n",
    "    return d1.groupby(CNAME_MONTH_OPENED).filter(lambda x: x.name == month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88331594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filePath):\n",
    "    '''\n",
    "    load the dataset, ISO-8859-9 encoding is used for characters specific to Turkish language.\n",
    "    '''\n",
    "    dataset = pandas.read_csv(filePath, encoding=\"ISO-8859-9\", delimiter=\";\") \n",
    "\n",
    "    # remove the spaces from the start and end of column names\n",
    "    dataset.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d8bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectTrainingDatasetRecords(dataset, train_year, train_month_list):\n",
    "    \"\"\"\n",
    "    filter issue records from the training dataset such that \n",
    "     * unresolved are eliminated and \n",
    "     * they are opened at specific time intervals\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = dataset[(dataset[CNAME_RECORD_TYPE] == FILTER_ISSUE_TYPE) &\n",
    "                      (dataset[CNAME_STATUS] == FILTER_ISSUE_STATUS)]\n",
    "    # # select year and month\n",
    "    frames = []\n",
    "    for train_month in train_month_list:\n",
    "        frames.append(selectRecordsOpenedAtYearMonth(dataset, train_year, train_month))\n",
    "\n",
    "    dataset = pandas.concat(frames)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37ab0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectTestDatasetRecords(dataset, test_year, test_month):\n",
    "    \"\"\"\n",
    "    filter issue records from the test dataset such that \n",
    "     * unresolved are eliminated and \n",
    "     * they are opened at specific time intervals\n",
    "\n",
    "    \"\"\"\n",
    "    # select year and month\n",
    "    frames = [selectRecordsOpenedAtYearMonth(dataset, test_year, test_month)]\n",
    "    dataset = pandas.concat(frames)\n",
    "    return dataset[(dataset[CNAME_RECORD_TYPE] == FILTER_ISSUE_TYPE) & \n",
    "                   (dataset[CNAME_STATUS] == FILTER_ISSUE_STATUS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32aba0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the stop words to be eliminated from the issue dataset \n",
    "# you can use your own stop-words instead of this list.\n",
    "stop_word_list = get_stop_words('turkish')\n",
    "#print(stop_word_list)\n",
    "\n",
    "# Turkish upper-case characters are lower-cased seperately so as to be sure of them\n",
    "lower_map_turkish = {\n",
    "    ord(u'I'): u'ı',\n",
    "    ord(u'İ'): u'i',\n",
    "    ord(u'Ç'): u'ç',\n",
    "    ord(u'Ş'): u'ş',\n",
    "    ord(u'Ö'): u'ö',\n",
    "    ord(u'Ü'): u'ü',\n",
    "    ord(u'Ğ'): u'ğ'\n",
    "}\n",
    "\n",
    "def filterNoise(text):\n",
    "    \"\"\"\n",
    "    converts words to lowercase, eliminates non-alphanumeric characters, eliminates stop-words\n",
    "    \"\"\"\n",
    "    # Remove all non-alphanumeric characters from the text via the regex[\\W]+,\n",
    "    # Convert the text into lowercase characters\n",
    "    try:\n",
    "        text_tr = text.translate(lower_map_turkish)\n",
    "        lowerText = re.sub('[\\W]+', ' ', text_tr.lower())\n",
    "    except AttributeError:\n",
    "        lowerText = \"\"\n",
    "    \n",
    "\n",
    "    #remove stopwords\n",
    "    noStopWordsText = [word for word in lowerText.split() if word not in stop_word_list]\n",
    "\n",
    "    return ' '.join(noStopWordsText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7e93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFileName = input(\"Please enter the name of the csv dataset to read:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56385bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "entireDataset = load(inputFileName)\n",
    "print(\"Entire dataset length: \" + str(len(entireDataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32ae2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_year = int(input(\"Please enter the issue report year to be included in train dataset: \"))\n",
    "train_month_list = []\n",
    "train_month = \"\"\n",
    "while (train_month != \"EXIT\"):\n",
    "    train_month = str(input(\"Please enter the issue report month to be included in train dataset (EXIT to stop): \"))\n",
    "    if (train_month != \"EXIT\"):\n",
    "        train_month_list.append(train_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "393cbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter training issue records\n",
    "trainDataset = selectTrainingDatasetRecords(entireDataset, train_year, train_month_list)\n",
    "#print(trainDataset[CNAME_ATTACHMENT_TEXT].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a0d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "trainDataset[CNAME_ATTACHMENT_TEXT] = trainDataset[CNAME_ATTACHMENT_TEXT].apply(filterNoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5a79b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to check training records\n",
    "print(\"Train dataset length : \" + str(len(trainDataset)))\n",
    "#print(trainDataset[CNAME_ATTACHMENT_TEXT].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14c7a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = int(input(\"Please enter the issue report year to be included in test dataset: \"))\n",
    "test_month = str(input(\"Please enter the issue report month to be included in test dataset: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb47958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter test issue records\n",
    "testDataset = selectTestDatasetRecords(entireDataset, test_year, test_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59f244e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing\n",
    "testDataset[CNAME_ATTACHMENT_TEXT] = testDataset[CNAME_ATTACHMENT_TEXT].apply(filterNoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "584d016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to check test records\n",
    "print(\"Test length: \" + str(len(testDataset)))\n",
    "#print(testDataset[CNAME_ATTACHMENT_TEXT].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c05ef856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input textual data to train (X_train) and related classes (Y_train)\n",
    "X_train = trainDataset[CNAME_ATTACHMENT_TEXT].values\n",
    "Y_train = trainDataset[CNAME_TEAMCODE].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "209e258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_a = TfidfVectorizer(ngram_range=(1,1), min_df=100, max_df=0.5)\n",
    "train_vectors_a = vectorizer_a.fit_transform(X_train)\n",
    "voc_a = vectorizer_a.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dee62b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input textual data to test (X_test) and related classes (Y_test)\n",
    "X_test = testDataset[CNAME_ATTACHMENT_TEXT].values\n",
    "Y_test = testDataset[CNAME_TEAMCODE].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15e0f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Td-idf conversion for test dataset\n",
    "vectorizer_test_a = TfidfVectorizer(ngram_range=(1,1), min_df=100, max_df=0.5, vocabulary=voc_a)\n",
    "X_tfidf_test = vectorizer_test_a.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb9f6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#       Test and Train with SVM_a_1  Model      #\n",
    "#################################################\n",
    "\n",
    "# Train with the LinearSVC model, calibrate to get probability results. \n",
    "Svm = LinearSVC()\n",
    "cSvm = CalibratedClassifierCV(Svm)\n",
    "cSvm.fit(train_vectors_a, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b7fd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction results\n",
    "predictions = cSvm.predict(X_tfidf_test)\n",
    "print(\"#################################################\")\n",
    "print(\"################ SVM_a_1 Results ################\")\n",
    "print(\"#################################################\")\n",
    "print(accuracy_score(Y_test, predictions))\n",
    "print(confusion_matrix(Y_test, predictions))\n",
    "print(classification_report(Y_test, predictions, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae49ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#       Test and Train with CNN_a_1  Model      #\n",
    "#################################################\n",
    "\n",
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, concatenate;\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "480d3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_1Channel(maxlen, max_features, embedding_dims, class_num, last_activation):\n",
    "    ###########################################################\n",
    "    # Returns the CNN model with single channel               #\n",
    "    # maxlen: Length of input sequences, when it is constant. #\n",
    "    #         This argument is required if you are going to   #\n",
    "    #         connect Flatten then Dense layers upstream      #\n",
    "    # max_features: size of the vocabulary                    #\n",
    "    # embedding_dims: dimension of the dense embedding        #\n",
    "    # class_num: number of classes (dimension of the keras    #\n",
    "    #         Dense output layer)                             #\n",
    "    # last_activation: Activation function to use for the     #\n",
    "    #         keras Dense layer                               #\n",
    "    ###########################################################\n",
    "    input = Input((maxlen,))\n",
    "    # Word embeddings\n",
    "    embedding = Embedding(max_features, embedding_dims, \n",
    "                          input_length=maxlen, trainable=True)(input)\n",
    "    # Convolutional layer and Max Pooling\n",
    "    convs = []\n",
    "    for kernel_size in [2, 3, 4]:\n",
    "        c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "        c = Dropout(0.5)(c)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        convs.append(c)\n",
    "    x = Concatenate()(convs)\n",
    "    # Fully connected layer\n",
    "    output = Dense(class_num, activation=last_activation)(x)\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a43ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class that transforma text data before CNN classification\n",
    "    def __init__(self): \n",
    "        # max_features: \n",
    "        self.max_features = 50000  # the maximum number of words to keep, based on word frequency.\n",
    "        self.max_length = 400      # maximum length of all sequences\n",
    "        # create the tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_features)\n",
    "        \n",
    "    def fit(self, X_train):\n",
    "        # fit the tokenizer on the documents\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "            \n",
    "    def transform(self, X_test):\n",
    "        # sequence encode\n",
    "        encoded_docs = self.tokenizer.texts_to_sequences(X_test)\n",
    "        # pad sequences\n",
    "        X_test_padded = pad_sequences(encoded_docs, maxlen=self.max_length, padding='post')\n",
    "\n",
    "        return X_test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64f34d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags = trainDataset[CNAME_TEAMCODE]\n",
    "test_tags = testDataset[CNAME_TEAMCODE]\n",
    "df = pandas.concat([trainDataset[CNAME_TEAMCODE], testDataset[CNAME_TEAMCODE]], axis=0)\n",
    "num_classes = df.nunique()\n",
    "print(num_classes)\n",
    "\n",
    "# Encode target labels with value between 0 and n_classes-1\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "# Convert class vector (integers) to binary class matrix.\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "#print(y_train[0])\n",
    "#print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a06c5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainDataset[CNAME_ATTACHMENT_TEXT].values\n",
    "vectorizer = TextTransformer()\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_test = testDataset[CNAME_ATTACHMENT_TEXT].values\n",
    "\n",
    "# pad sequences\n",
    "X_train_padded = vectorizer.transform(X_train)\n",
    "X_test_padded = vectorizer.transform(X_test)\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(vectorizer.tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "embedding_dims = 300\n",
    "max_length = 400\n",
    "last_activation='softmax'\n",
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb50929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1channel = get_model_1Channel(max_length, vocab_size, embedding_dims, \n",
    "                                    class_num=num_classes, last_activation=last_activation)\n",
    "model_1channel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_1channel.fit(X_train_padded, y_train, \n",
    "                   batch_size=batch_size, \n",
    "                   epochs=epochs, \n",
    "                   validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac3c3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = model_1channel.predict(X_test_padded)\n",
    "print(\"#################################################\")\n",
    "print(\"################ CNN_a_1 Results ################\")\n",
    "print(\"#################################################\")\n",
    "\n",
    "predictions = []\n",
    "for proba in pred_proba:\n",
    "    proba_list = list(proba)\n",
    "    index = proba_list.index(max(proba_list))\n",
    "    predictions.append(index)\n",
    "\n",
    "Y_validation = []\n",
    "for y in y_test:\n",
    "    y_list = list(y)\n",
    "    index = y_list.index(max(y_list))\n",
    "    Y_validation.append(index)\n",
    "\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c1db52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#       Test and Train with VGG_a_1  Model      #\n",
    "#################################################\n",
    "from matplotlib import pyplot\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c253054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define VGG model\n",
    "def define_VGG_model(class_mode, num_classes):\n",
    "    # load model\n",
    "    model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
    "    # mark loaded layers as trainable / not trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # add new classifier layers\n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "    output = Dense(num_classes, activation='softmax')(class1) #softmax?\n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss=class_mode+'_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15d5b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def train_test_with_images(train_data_path, test_data_path, class_mode, num_classes, epochs, VGG_flag):\n",
    "    # define model\n",
    "    if VGG_flag: \n",
    "        model = define_VGG_model(class_mode, num_classes)\n",
    "    else:\n",
    "        model = define_model(class_mode, num_classes)\n",
    "    # create data generator\n",
    "    datagen = ImageDataGenerator(featurewise_center=True)\n",
    "    # specify imagenet mean values for centering\n",
    "    datagen.mean = [123.68, 116.779, 103.939]\n",
    "    # prepare iterator\n",
    "    train_it = datagen.flow_from_directory(train_data_path, class_mode=class_mode, \n",
    "                                           batch_size=64, target_size=(224, 224))\n",
    "    test_it = datagen.flow_from_directory(test_data_path, class_mode=class_mode, shuffle=False, \n",
    "                                          batch_size=64, target_size=(224, 224))\n",
    "    # fit model\n",
    "    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
    "                                  validation_data=test_it, validation_steps=len(test_it), epochs=epochs, verbose=0)\n",
    "\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "\n",
    "    #Confusion Matrix and Classification Report\n",
    "    Y_pred = model.predict_generator(test_it)\n",
    "    print(Y_pred)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print(y_pred)\n",
    "    print(\"=================\")\n",
    "    print(test_it.classes)\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(test_it.classes, y_pred))\n",
    "    print('Classification Report')\n",
    "    print(classification_report(test_it.classes, y_pred, digits=5))\n",
    "    print('Accuracy Score')\n",
    "    print(accuracy_score(test_it.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83311e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#################################################\")\n",
    "print(\"################ VGG_a_1 Results ################\")\n",
    "print(\"#################################################\")\n",
    "\n",
    "# training and test images should be in the following folders according \n",
    "# to the related classes (in other words teams they are assigned).\n",
    "train_data_path = 'train_images'\n",
    "test_data_path = 'test_images'\n",
    "# num_classes has been set before, reset if necessary\n",
    "class_mode = 'categorical' #binary or categorical\n",
    "epochs = 10\n",
    "train_test_with_images(train_data_path, test_data_path, class_mode, num_classes, epochs, VGG_flag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
